# test_task_parsing_site

## 1. Что было сделано?

Были получены данные с сайта цитат (https://quotes.toscrape.com/), сформирован json файл следующего вида:

quotes = [

    {
    
        "quote": str,
        
        "author": str,
        
        "tags": [
        
            str,
            
            ...
            
        ]
        
        ...
        
    }
    
]

Список словарей, где каждый словарь является чей-то цитатой. В этих словарях одинаковая струкрура: три ключа(quote, author, tags), где tags является списком тэгов.

## 2. Откуда были получены данные?

Данные взяты со страниц: 

https://quotes.toscrape.com/page/1/ ... https://quotes.toscrape.com/page/10/

## 3. Как осуществлялся сбор?
   
- С помощью библиотеки requests был получен ответ на GET запрос к первой странице (https://quotes.toscrape.com/page/1/). Если статус ответа был равен 200, то с помощью библиотеки BeautifulSoup из этого ответа было сформировано дерево веб-страницы. 
- Работая с этим деревом, были получены данные 'quote', 'author' и 'tags'. В цикле формируется словарь и добавляется в общий список цитат.
- Дальше идет проверка наличия на странице ссылки ('li', class_='next'), которая ведет на следующую страницу. Если следующая страница есть, то меняем url и работаем с новой страницей по тому же алгоритму.
- Когда страницы с next закончатся, будет совершен выход из цикла.
- После цикла идет преобразование списка цитат в json формат и создание файла 'quotes.json'.

## 4. Почему был выбран тот или иной метод/инструмент, а не другой?
В данном случае сайт был с достаточно простой структурой, маленьким объемом информации, а выгрузка данных производилась единожды. Такие инструменты, как scrapy или selenium слишком массивные фреймворки и использовать их в данном случае не самая лучшая идея.
Поэтому были выбраны requests и BeautifulSoup.
